"""
ìƒì„¸ íˆ¬ìì ë°ì´í„° í†µí•© íŒŒì¼ ìƒì„±
- ê°œë³„ parquet íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ í†µí•© íŒŒì¼ë¡œ ë³‘í•©
- 48ê°œ ì»¬ëŸ¼ (ê¸°ì¡´ 40ê°œ + ìƒˆë¡œìš´ 7ê°œ + ë‚ ì§œ)
"""
import pandas as pd
from pathlib import Path
from datetime import datetime


def consolidate_all_stocks():
    """ëª¨ë“  ì¢…ëª© ë°ì´í„°ë¥¼ í†µí•© íŒŒì¼ë¡œ ë³‘í•©"""
    
    data_dir = Path('./data/crash_rebound')
    output_file = data_dir / 'all_stocks_3years.parquet'
    
    # ê°œë³„ íŒŒì¼ ëª©ë¡
    stock_files = list(data_dir.glob('*_*.parquet'))
    stock_files = [f for f in stock_files if f.name != 'all_stocks_3years.parquet']
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š í†µí•© ë°ì´í„° íŒŒì¼ ìƒì„±")
    print(f"{'='*60}")
    print(f"ê°œë³„ íŒŒì¼ ìˆ˜: {len(stock_files)}ê°œ")
    print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"{'='*60}\n")
    
    all_data = []
    
    for i, file_path in enumerate(stock_files, 1):
        try:
            # íŒŒì¼ ë¡œë“œ
            df = pd.read_parquet(file_path)
            
            # ì¢…ëª© ì •ë³´ ì¶”ê°€
            parts = file_path.stem.split('_')
            if len(parts) >= 2:
                stock_code = parts[0]
                stock_name = '_'.join(parts[1:])
                
                df['stock_code'] = stock_code
                df['stock_name'] = stock_name
                
                all_data.append(df)
                
                if i % 100 == 0:
                    print(f"[{i}/{len(stock_files)}] ë¡œë“œ ì¤‘... (ì´ {sum(len(d) for d in all_data):,}í–‰)")
                    
        except Exception as e:
            print(f"âŒ {file_path.name}: {e}")
    
    print(f"\nâœ… ëª¨ë“  íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    print(f"ì´ {len(all_data)}ê°œ ì¢…ëª©")
    
    # í†µí•©
    print("\nğŸ”„ ë°ì´í„° ë³‘í•© ì¤‘...")
    df_all = pd.concat(all_data, ignore_index=False)
    
    # ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜
    df_all = df_all.reset_index()
    
    # Date ì»¬ëŸ¼ í™•ì¸ ë° ì •ë¦¬
    if 'index' in df_all.columns and 'Date' not in df_all.columns:
        df_all = df_all.rename(columns={'index': 'Date'})
    elif 'index' in df_all.columns and 'Date' in df_all.columns:
        df_all = df_all.drop(columns=['index'])
    
    # ì •ë ¬
    if 'Date' in df_all.columns:
        df_all = df_all.sort_values(['stock_code', 'Date'])
    else:
        df_all = df_all.sort_values('stock_code')
    
    print(f"\nğŸ“Š í†µí•© ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ í–‰ ìˆ˜: {len(df_all):,}")
    print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(df_all.columns)}")
    print(f"   - ì¢…ëª© ìˆ˜: {df_all['stock_code'].nunique()}")
    print(f"   - ë‚ ì§œ ë²”ìœ„: {df_all['Date'].min()} ~ {df_all['Date'].max()}")
    
    # ì»¬ëŸ¼ í™•ì¸
    new_cols = ['financial_invest_net', 'insurance_net', 'fund_net', 
                'private_fund_net', 'bank_net', 'other_finance_net', 'pension_net']
    
    print(f"\nâœ… ìƒˆë¡œìš´ ì»¬ëŸ¼:")
    for col in new_cols:
        if col in df_all.columns:
            non_zero = (df_all[col] != 0).sum()
            pct = non_zero / len(df_all) * 100
            print(f"   {col}: {non_zero:,} / {len(df_all):,} ({pct:.1f}%)")
        else:
            print(f"   âŒ {col}: ì—†ìŒ")
    
    # ì €ì¥
    print(f"\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘: {output_file}")
    df_all.to_parquet(output_file, index=False, compression='snappy')
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = output_file.stat().st_size / (1024 * 1024)
    print(f"âœ… ì €ì¥ ì™„ë£Œ! (í¬ê¸°: {file_size_mb:.1f} MB)")
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ í†µí•© íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print(f"{'='*60}\n")
    
    return df_all


def verify_data_quality(df):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
    print(f"{'='*60}\n")
    
    # 1. ê¸°ë³¸ ì •ë³´
    print("1. ê¸°ë³¸ ì •ë³´")
    print(f"   - ì „ì²´ í–‰ ìˆ˜: {len(df):,}")
    print(f"   - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"   - ì¢…ëª© ìˆ˜: {df['stock_code'].nunique()}")
    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    
    # 2. ê²°ì¸¡ì¹˜ í™•ì¸
    print(f"\n2. ê²°ì¸¡ì¹˜ (ìƒìœ„ 5ê°œ)")
    missing = df.isnull().sum()
    missing = missing[missing > 0].sort_values(ascending=False).head()
    if len(missing) > 0:
        for col, count in missing.items():
            pct = count / len(df) * 100
            print(f"   {col}: {count:,} ({pct:.1f}%)")
    else:
        print("   âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
    
    # 3. ê¸‰ë½ ì´ë²¤íŠ¸ í†µê³„
    print(f"\n3. ê¸‰ë½ ì´ë²¤íŠ¸")
    crashes = df[df['crash'] == 1]
    print(f"   - ê¸‰ë½ ì´ë²¤íŠ¸ ìˆ˜: {len(crashes):,}")
    print(f"   - ê¸‰ë½ë¥ : {len(crashes) / len(df) * 100:.2f}%")
    
    if 'success' in df.columns:
        success_rate = crashes['success'].mean() * 100
        print(f"   - ë°˜ë“± ì„±ê³µë¥ : {success_rate:.1f}%")
    
    # 4. ìƒˆë¡œìš´ íˆ¬ìì ë°ì´í„° í†µê³„
    print(f"\n4. ìƒì„¸ íˆ¬ìì ë°ì´í„°")
    new_cols = ['financial_invest_net', 'fund_net', 'pension_net']
    for col in new_cols:
        if col in df.columns:
            non_zero = (df[col] != 0).sum()
            pct = non_zero / len(df) * 100
            mean_val = df[df[col] != 0][col].mean()
            print(f"   {col}:")
            print(f"      ë°ì´í„°ìœ¨: {pct:.1f}%")
            print(f"      í‰ê· ê°’: {mean_val:,.0f}")
    
    # 5. ì¢…ëª©ë³„ í†µê³„
    print(f"\n5. ì¢…ëª©ë³„ ë°ì´í„° ìˆ˜")
    stock_counts = df.groupby('stock_code').size()
    print(f"   - í‰ê· : {stock_counts.mean():.0f}ì¼")
    print(f"   - ìµœì†Œ: {stock_counts.min()}ì¼")
    print(f"   - ìµœëŒ€: {stock_counts.max()}ì¼")
    
    print(f"\n{'='*60}")
    print(f"âœ… ê²€ì¦ ì™„ë£Œ")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    # í†µí•© íŒŒì¼ ìƒì„±
    df = consolidate_all_stocks()
    
    # í’ˆì§ˆ ê²€ì¦
    verify_data_quality(df)
    
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ìƒê´€ê´€ê³„ ë¶„ì„: python analysis/correlation_analysis.py")
    print("2. AI ëª¨ë¸ ì¬í•™ìŠµ: python ai_model/train_crash_rebound.py")
