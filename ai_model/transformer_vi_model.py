"""
Transformer ê¸°ë°˜ VI ë°˜ë“± ì˜ˆì¸¡ ëª¨ë¸
- Input: VI ì „í›„ 60ì´ˆ í‹± ë°ì´í„° (ì‹œê³„ì—´)
- Output: ë°˜ë“± í™•ë¥ , ì˜ˆìƒ ìˆ˜ìµë¥ , ìµœì  ì§„ì… íƒ€ì´ë°
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from pathlib import Path


class VISequenceDataset(Dataset):
    """VI ì´ë²¤íŠ¸ ì‹œê³„ì—´ ë°ì´í„°ì…‹"""
    
    def __init__(self, vi_events_dir, sequence_length=60):
        """
        Args:
            vi_events_dir: VI ì´ë²¤íŠ¸ ë””ë ‰í† ë¦¬ (ê° íŒŒì¼ = 1 VI ì´ë²¤íŠ¸)
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´ (ì´ˆ ë‹¨ìœ„, ì „í›„ 60ì´ˆ = 120ì´ˆ)
        """
        self.sequence_length = sequence_length
        self.vi_files = list(Path(vi_events_dir).glob('*.parquet'))
        
        print(f"âœ… {len(self.vi_files)}ê°œ VI ì´ë²¤íŠ¸ ë¡œë“œ")
    
    def __len__(self):
        return len(self.vi_files)
    
    def __getitem__(self, idx):
        """
        Returns:
            features: (120, 21) - 120ì´ˆ Ã— 21ê°œ feature
            label: (3,) - [ë°˜ë“±ì—¬ë¶€, ìˆ˜ìµë¥ , ì§„ì…íƒ€ì´ë°]
        """
        # VI ì´ë²¤íŠ¸ íŒŒì¼ ë¡œë“œ
        df = pd.read_parquet(self.vi_files[idx])
        
        # Feature ì¶”ì¶œ
        features = self._extract_features(df)
        
        # Label ì¶”ì¶œ (VI í•´ì œ í›„ 30ì´ˆ ê¸°ì¤€)
        label = self._extract_label(df)
        
        return torch.FloatTensor(features), torch.FloatTensor(label)
    
    def _extract_features(self, df):
        """
        21ê°œ feature ì¶”ì¶œ
        
        Features:
          1. ì •ê·œí™”ëœ ê°€ê²© (close_norm)
          2. ê±°ë˜ëŸ‰ (volume_norm)
          3-12. ë§¤ìˆ˜í˜¸ê°€ 1~10 (bid_price_1~10_norm)
          13-22. ë§¤ë„í˜¸ê°€ 1~10 (ask_price_1~10_norm)
          23. í”„ë¡œê·¸ë¨ ìˆœë§¤ìˆ˜ (program_net_norm)
          24. ê¸°ê´€ ìˆœë§¤ìˆ˜ (institution_net_norm)
          25. ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ (foreign_net_norm)
          26. VI ìƒíƒœ (0=ì •ìƒ, 1=ë°œë™, 2=í•´ì œ)
        
        Returns:
            (120, 26) numpy array
        """
        # 120ì´ˆ ìŠ¬ë¼ì´ì‹± (VI ì „ 60ì´ˆ + í›„ 60ì´ˆ)
        vi_idx = df[df['vi_status'] == 1].index[0]  # VI ë°œë™ ì‹œì 
        start_idx = max(0, vi_idx - 60)
        end_idx = min(len(df), vi_idx + 60)
        
        df_slice = df.iloc[start_idx:end_idx]
        
        # Feature ì •ê·œí™”
        features = []
        
        # ê°€ê²© (MinMax ì •ê·œí™”)
        price_norm = (df_slice['price'] - df_slice['price'].min()) / \
                     (df_slice['price'].max() - df_slice['price'].min() + 1e-8)
        features.append(price_norm.values)
        
        # ê±°ë˜ëŸ‰ (Log ì •ê·œí™”)
        volume_norm = np.log1p(df_slice['volume']) / 10.0
        features.append(volume_norm.values)
        
        # í˜¸ê°€ (ìƒëŒ€ì  ê±°ë¦¬)
        for i in range(1, 11):
            bid_norm = (df_slice[f'bid_price_{i}'] - df_slice['price']) / df_slice['price']
            ask_norm = (df_slice[f'ask_price_{i}'] - df_slice['price']) / df_slice['price']
            features.append(bid_norm.values)
            features.append(ask_norm.values)
        
        # í”„ë¡œê·¸ë¨/ê¸°ê´€/ì™¸êµ­ì¸ (í‘œì¤€í™”)
        for col in ['program_net_buy', 'institution_net', 'foreign_net']:
            values = df_slice[col].values
            norm = (values - values.mean()) / (values.std() + 1e-8)
            features.append(norm)
        
        # VI ìƒíƒœ (ì›í•« ì¸ì½”ë”©)
        vi_status = df_slice['vi_status'].values / 2.0  # 0~1 ë²”ìœ„
        features.append(vi_status)
        
        # (26, 120) â†’ (120, 26) ì „ì¹˜
        features = np.array(features).T
        
        # 120ì´ˆ ë¯¸ë§Œì´ë©´ íŒ¨ë”©
        if len(features) < 120:
            pad = np.zeros((120 - len(features), 26))
            features = np.vstack([features, pad])
        
        return features[:120]  # ì •í™•íˆ 120ì´ˆ
    
    def _extract_label(self, df):
        """
        Label ì¶”ì¶œ
        
        Returns:
            [ë°˜ë“±ì—¬ë¶€, ìˆ˜ìµë¥ , ì§„ì…íƒ€ì´ë°]
            
            - ë°˜ë“±ì—¬ë¶€: VI í•´ì œ í›„ 30ì´ˆ ë‚´ 2% ì´ìƒ ìƒìŠ¹ â†’ 1, ì•„ë‹ˆë©´ 0
            - ìˆ˜ìµë¥ : VI í•´ì œê°€ ëŒ€ë¹„ 30ì´ˆ í›„ ìµœê³ ê°€ ìˆ˜ìµë¥ 
            - ì§„ì…íƒ€ì´ë°: ìµœê³  ìˆ˜ìµë¥ ì„ ê¸°ë¡í•œ ì‹œì  (0~29ì´ˆ)
        """
        vi_idx = df[df['vi_status'] == 1].index[0]
        release_idx = df[df['vi_status'] == 2].index[0]  # VI í•´ì œ
        
        # VI í•´ì œ í›„ 30ì´ˆ
        after_release = df.iloc[release_idx:release_idx+30]
        
        if len(after_release) == 0:
            return np.array([0.0, 0.0, 0.0])
        
        entry_price = df.iloc[release_idx]['price']
        max_price = after_release['price'].max()
        max_idx = after_release['price'].idxmax()
        
        # ìˆ˜ìµë¥ 
        profit_rate = (max_price - entry_price) / entry_price
        
        # ë°˜ë“± ì—¬ë¶€ (2% ì´ìƒ)
        rebound = 1.0 if profit_rate > 0.02 else 0.0
        
        # ì§„ì… íƒ€ì´ë° (0~29ì´ˆ)
        entry_timing = min(max_idx - release_idx, 29) / 29.0  # ì •ê·œí™”
        
        return np.array([rebound, profit_rate, entry_timing])


class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”© (ì‹œê³„ì—´ ìˆœì„œ ì •ë³´)"""
    
    def __init__(self, d_model, max_len=120):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                             (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]


class VITransformerModel(nn.Module):
    """
    Transformer ê¸°ë°˜ VI ë°˜ë“± ì˜ˆì¸¡ ëª¨ë¸
    
    Architecture:
      1. Input Embedding: (120, 26) â†’ (120, 256)
      2. Positional Encoding
      3. Transformer Encoder (6 layers)
      4. Multi-Head Output:
         - ë°˜ë“± í™•ë¥  (Binary Classification)
         - ì˜ˆìƒ ìˆ˜ìµë¥  (Regression)
         - ì§„ì… íƒ€ì´ë° (Regression, 0~1)
    """
    
    def __init__(self, input_dim=26, d_model=256, nhead=8, num_layers=6, dropout=0.1):
        super().__init__()
        
        # Input Embedding
        self.embedding = nn.Linear(input_dim, d_model)
        
        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=1024,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Global Average Pooling
        self.pool = nn.AdaptiveAvgPool1d(1)
        
        # Multi-Head Output
        self.rebound_head = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1),
            nn.Sigmoid()  # í™•ë¥  (0~1)
        )
        
        self.profit_head = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1)  # ìˆ˜ìµë¥  (ì‹¤ìˆ˜)
        )
        
        self.timing_head = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1),
            nn.Sigmoid()  # íƒ€ì´ë° (0~1)
        )
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, 120, 26) - VI ì „í›„ 60ì´ˆ ì‹œê³„ì—´
        
        Returns:
            dict: {
                'rebound_prob': (batch_size, 1),
                'expected_profit': (batch_size, 1),
                'entry_timing': (batch_size, 1)
            }
        """
        # Embedding
        x = self.embedding(x)  # (B, 120, 256)
        
        # Positional Encoding
        x = self.pos_encoder(x)
        
        # Transformer
        x = self.transformer(x)  # (B, 120, 256)
        
        # Global Average Pooling
        x = x.transpose(1, 2)  # (B, 256, 120)
        x = self.pool(x).squeeze(-1)  # (B, 256)
        
        # Multi-Head Prediction
        rebound_prob = self.rebound_head(x)
        expected_profit = self.profit_head(x)
        entry_timing = self.timing_head(x)
        
        return {
            'rebound_prob': rebound_prob,
            'expected_profit': expected_profit,
            'entry_timing': entry_timing
        }


class VIModelTrainer:
    """ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        
        # ë©€í‹°íƒœìŠ¤í¬ ì†ì‹¤ í•¨ìˆ˜
        self.criterion_rebound = nn.BCELoss()  # ë°˜ë“± ì—¬ë¶€
        self.criterion_profit = nn.MSELoss()   # ìˆ˜ìµë¥ 
        self.criterion_timing = nn.MSELoss()   # íƒ€ì´ë°
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(  # type: ignore
            model.parameters(),
            lr=1e-4,
            weight_decay=1e-5
        )
        
        # Learning Rate Scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100
        )
    
    def train_epoch(self, train_loader):
        """1 Epoch í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            features = features.to(self.device)
            labels = labels.to(self.device)
            
            # Forward
            outputs = self.model(features)
            
            # Loss ê³„ì‚° (ë©€í‹°íƒœìŠ¤í¬)
            loss_rebound = self.criterion_rebound(
                outputs['rebound_prob'], 
                labels[:, 0:1]
            )
            loss_profit = self.criterion_profit(
                outputs['expected_profit'], 
                labels[:, 1:2]
            )
            loss_timing = self.criterion_timing(
                outputs['entry_timing'], 
                labels[:, 2:3]
            )
            
            # ê°€ì¤‘ í•©ì‚°
            loss = loss_rebound * 0.5 + loss_profit * 0.3 + loss_timing * 0.2
            
            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        self.scheduler.step()
        return total_loss / len(train_loader)
    
    def evaluate(self, val_loader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(features)
                
                # ë°˜ë“± ì˜ˆì¸¡ ì •í™•ë„
                pred_rebound = (outputs['rebound_prob'] > 0.5).float()
                correct += (pred_rebound == labels[:, 0:1]).sum().item()
                total += labels.size(0)
                
                # Loss
                loss_rebound = self.criterion_rebound(
                    outputs['rebound_prob'], labels[:, 0:1]
                )
                loss_profit = self.criterion_profit(
                    outputs['expected_profit'], labels[:, 1:2]
                )
                loss_timing = self.criterion_timing(
                    outputs['entry_timing'], labels[:, 2:3]
                )
                
                loss = loss_rebound * 0.5 + loss_profit * 0.3 + loss_timing * 0.2
                total_loss += loss.item()
        
        accuracy = correct / total
        avg_loss = total_loss / len(val_loader)
        
        return avg_loss, accuracy


def main():
    """í•™ìŠµ ì‹¤í–‰ ì˜ˆì œ"""
    
    # Device ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"âœ… Device: {device}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ ê²½ë¡œ)
    train_dataset = VISequenceDataset('./data/vi_events/train')
    val_dataset = VISequenceDataset('./data/vi_events/val')
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # ëª¨ë¸ ìƒì„±
    model = VITransformerModel(
        input_dim=26,
        d_model=256,
        nhead=8,
        num_layers=6
    )
    
    print(f"âœ… ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = VIModelTrainer(model, device=device)
    
    # í•™ìŠµ
    best_accuracy = 0
    for epoch in range(100):
        train_loss = trainer.train_epoch(train_loader)
        val_loss, val_accuracy = trainer.evaluate(val_loader)
        
        print(f"Epoch {epoch+1}/100:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}")
        print(f"  Val Accuracy: {val_accuracy:.2%}")
        
        # Best ëª¨ë¸ ì €ì¥
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), './models/vi_transformer_best.pth')
            print(f"  âœ… Best ëª¨ë¸ ì €ì¥ (ì •í™•ë„: {best_accuracy:.2%})")
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ìµœê³  ì •í™•ë„: {best_accuracy:.2%}")


if __name__ == '__main__':
    main()
